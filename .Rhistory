plot(testHands)
plot(pred)
plot(pred,xlab="Hands by Numeric Class", ylab="Predictions",main="K-NN Predictions")
#initial setup for data analysis
#reading the datasets, creat and format the training and test datasets
#read in the poker data sets and store them in two datasets
train <- read.csv("~/PokerHands/poker-hand-training-true.data", header=FALSE)
test <- read.csv("~/PokerHands/poker-hand-testing.data", header=FALSE)
#give the columns names
names(train) <- c("S1","R1","S2","R2","S3","R3","S4","R4","S5","R5","Hand")
names(test) <- c("S1","R1","S2","R2","S3","R3","S4","R4","S5","R5","Hand")
#convert the the Class variable Hand into a factor
train$Hand <- as.factor(train$Hand)
test$Hand <- as.factor(test$Hand)
#create a training set with equal populations for all classes to resolve the class imbalance inherant in poker hands
downSampleTrain <- downSample(train[,1:10], train$Hand, list = FALSE, yname = "Hand")
#libraries need for this exploration
library(e1071)
library(caret)
library(NLP)
library(tm)
library(pROC)
library(lattice)
library(ada)
library(rpart)
library(kernlab)
library(ipred)
library(plyr)
library(MASS)
library(pamr)
library(randomForest)
###################################################################################
#initial setup for data analysis
#reading the datasets, creat and format the training and test datasets
#read in the poker data sets and store them in two datasets
train <- read.csv("~/PokerHands/poker-hand-training-true.data", header=FALSE)
test <- read.csv("~/PokerHands/poker-hand-testing.data", header=FALSE)
#give the columns names
names(train) <- c("S1","R1","S2","R2","S3","R3","S4","R4","S5","R5","Hand")
names(test) <- c("S1","R1","S2","R2","S3","R3","S4","R4","S5","R5","Hand")
#convert the the Class variable Hand into a factor
train$Hand <- as.factor(train$Hand)
test$Hand <- as.factor(test$Hand)
#create a training set with equal populations for all classes to resolve the class imbalance inherant in poker hands
downSampleTrain <- downSample(train[,1:10], train$Hand, list = FALSE, yname = "Hand")
upSampleTrain <- upSample(train[,1:10], train$Hand, list = FALSE, yname = "Hand")
#separate the attributes from the labels for model training
x <- upSampleTrain[,1:10]
y <- as.factor(upSampleTrain$Hand)
barchart(table(train$Hand),main="Hand distribution")
barchart(table(upSampleTrain$Hand),main="Hand distribution after Up Sampling")
par(mfrow=c(1,2))
barchart(table(downSampleTrain$Hand),main="Hand distribution")
barchart(table(upSampleTrain$Hand),main="Hand distribution after Up Sampling")
par(mfrow=c(2,1))
barchart(table(downSampleTrain$Hand),main="Hand distribution")
barchart(table(upSampleTrain$Hand),main="Hand distribution after Up Sampling")
# One figure in row 1 and two figures in row 2
layout(matrix(c(1,1,2,3), 2, 2, byrow = TRUE))
barchart(table(train$Hand))
barchart(table(downSampleTrain$Hand),main="Hand distribution")
barchart(table(upSampleTrain$Hand),main="Hand distribution after Up Sampling")
barchart(table(downSampleTrain$Hand),main="Hand distribution Down Sampling")
#an exploration of Machine learning Algorithms in the caret package
#on the Poker Hands dataset
#this attempt uses the caret pack to expore the tuning options of the models choosen
#this attempt will create a training set with equal populations for all classes to resolve the class population imbalance inherant in poker hands, but they will be tested against a sample that refects those imbalances
#reading the datasets
#read in the poker data sets and store them in two datasets
train <- read.csv("~/PokerHands/poker-hand-training-true.data", header=FALSE)
test <- read.csv("~/PokerHands/poker-hand-testing.data", header=FALSE)
#give the columns names
names(train) <- c("S1","R1","S2","R2","S3","R3","S4","R4","S5","R5","Hand")
names(test) <- c("S1","R1","S2","R2","S3","R3","S4","R4","S5","R5","Hand")
#convert the the Class variable Hand into a factor
train$Hand <- as.factor(train$Hand)
test$Hand <- as.factor(test$Hand)
#create a training set with equal populations for all classes to resolve the class imbalance inherant in poker hands
downSampleTrain <- downSample(train[,1:10], train$Hand, list = FALSE, yname = "Hand")
upSampleTrain <- upSample(train[,1:10], train$Hand, list = FALSE, yname = "Hand")
#separate the attributes from the labels
x <- upSampleTrain[,1:10]
y <- as.factor(upSampleTrain$Hand)
#define the resampling method
fitControl <- trainControl(method = "cv",number =10)
#define parameters for tuning
tgrid <- expand.grid(fL = 0 ,usekernel = FALSE)
set.seed(12)
fitNB <- train(x,y, method ="nb",trContronl=fitControl,tuneGrid=tgrid)
#an exploration of Machine learning Algorithms in the caret package
#on the Poker Hands dataset
#this attempt uses the caret pack to expore the tuning options of the models choosen
#this attempt will create a training set with equal populations for all classes to resolve the class population imbalance inherant in poker hands, but they will be tested against a sample that refects those imbalances
#reading the datasets
#read in the poker data sets and store them in two datasets
train <- read.csv("~/PokerHands/poker-hand-training-true.data", header=FALSE)
test <- read.csv("~/PokerHands/poker-hand-testing.data", header=FALSE)
#give the columns names
names(train) <- c("S1","R1","S2","R2","S3","R3","S4","R4","S5","R5","Hand")
names(test) <- c("S1","R1","S2","R2","S3","R3","S4","R4","S5","R5","Hand")
#convert the the Class variable Hand into a factor
train$Hand <- as.factor(train$Hand)
test$Hand <- as.factor(test$Hand)
#create a training set with equal populations for all classes to resolve the class imbalance inherant in poker hands
downSampleTrain <- downSample(train[,1:10], train$Hand, list = FALSE, yname = "Hand")
upSampleTrain <- upSample(train[,1:10], train$Hand, list = FALSE, yname = "Hand")
#separate the attributes from the labels
x <- upSampleTrain[,1:10]
y <- as.factor(upSampleTrain$Hand)
#an exploration of Machine learning Algorithms in the caret package
#on the Poker Hands dataset
#this attempt uses the caret pack to expore the tuning options of the models choosen
#this attempt will create a training set with equal populations for all classes to resolve the class population imbalance inherant in poker hands, but they will be tested against a sample that refects those imbalances
#library's need for this investigation
library(caret)
#reading the datasets
#read in the poker data sets and store them in two datasets
train <- read.csv("~/PokerHands/poker-hand-training-true.data", header=FALSE)
test <- read.csv("~/PokerHands/poker-hand-testing.data", header=FALSE)
#give the columns names
names(train) <- c("S1","R1","S2","R2","S3","R3","S4","R4","S5","R5","Hand")
names(test) <- c("S1","R1","S2","R2","S3","R3","S4","R4","S5","R5","Hand")
#convert the the Class variable Hand into a factor
train$Hand <- as.factor(train$Hand)
test$Hand <- as.factor(test$Hand)
#create a training set with equal populations for all classes to resolve the class imbalance inherant in poker hands
downSampleTrain <- downSample(train[,1:10], train$Hand, list = FALSE, yname = "Hand")
upSampleTrain <- upSample(train[,1:10], train$Hand, list = FALSE, yname = "Hand")
#separate the attributes from the labels
x <- upSampleTrain[,1:10]
y <- as.factor(upSampleTrain$Hand)
###########################################
# Naive Bayes                             #
#define the resampling method
fitControl <- trainControl(method = "cv",number =10)
#define parameters for tuning
tgrid <- expand.grid(fL = 0 ,usekernel = FALSE)
set.seed(12)
fitNB <- train(x,y, method ="nb",trContronl=fitControl,tuneGrid=tgrid)
library(klaR)
library(MASS)
##############################
# Neural Net                 #
#library's need for this investigation
library(nnet)
library(neuralnet)
library(devtools)
library(caret)
library(klaR)
library(MASS)
#web reasource to plot neural nets
source_url('https://gist.githubusercontent.com/fawda123/7471137/raw/466c1474d0a505ff044412703516c34f1a4684a5/nnet_plot_update.r')
#define the resampling method
fitControl <- trainControl(method = "none")
#define parameters for tuning
tgrid <- expand.grid(size = 4 ,decay = 0.1)
set.seed(12)
fitNN <- train(x,y, method ="nnet",trContronl=fitControl,tuneGrid=tgrid)
###########################################
# Naive Bayes                             #
#library's need for this investigation
library(MASS)
library(klaR)
#define the resampling method
fitControl <- trainControl(method = "cv",number =10)
#define parameters for tuning
tgrid <- expand.grid(fL = 0 ,usekernel = FALSE)
set.seed(12)
fitNB <- train(x,y, method ="nb",trContronl=fitControl,tuneGrid=tgrid)
pred <- predict(fitNB, newdata=test[,1:10] )
CM <- confusionMatrix(pred,test$Hand)
##########################################
#Boosted Classification Trees            #
#library's need for this investigation
library(caret)
library(rattle)
library(rpart.plot)
library(RColorBrewer)
library(ipred)
library(plyr)
library(vcd)
library(lattice)
#define the resampling method
fitControl <- trainControl(method = "none")
#define parameters for tuning
tgrid <- expand.grid(iter = 50, maxdepth = 5 , nu = 1)
tgrid2 <- expand.grid(n.trees = c(10), interaction.depth=c(2) ,shrinkage=c(0.01),n.minobsinnode = c(2) )
#TreeBag model
set.seed(12)
fit <- train(x,y,method ="treebag")
fitTreeBag <- fit
pred <- predict(fit, newdata=test[,1:10])
treeBagCM <- confusionMatrix(pred,test$Hand)
#boosting Tree model
set.seed(12)
fit <- train(x,y,method ="gbm",trContronl=fitControl,tuneGrid=tgrid2)
fitBoost <- fit
pred <- predict(fitBoost, newdata=test[,1:10])
BoostCM <- confusionMatrix(pred,test$Hand)
BoostCM
##############################
# Neural Net                 #
#library's need for this investigation
library(nnet)
library(neuralnet)
library(devtools)
library(caret)
#web reasource to plot neural nets
source_url('https://gist.githubusercontent.com/fawda123/7471137/raw/466c1474d0a505ff044412703516c34f1a4684a5/nnet_plot_update.r')
#define the resampling method
fitControl <- trainControl(method = "none")
#define parameters for tuning
tgrid <- expand.grid(size = 4 ,decay = 0.1)
set.seed(12)
fitNN <- train(x,y, method ="nnet",trContronl=fitControl,tuneGrid=tgrid)
plot.nnet(fitNN)
#create a predictor based on the Neural Network and our testing data
pred <- predict(fitNN, newdata=test[,1:10])
neuralNetworkCM = confusionMatrix(pred,test$Hand)
neuralNetworkCM
##############################
analysis[[8]][1]
analysis[[9]][1]
analysis[[8]][4]
analysis[[9]][1]
analysis[[9]][4]
analysis[[10]][4]
analysis[[9]][4]
plot(fitTreeBag)
fitTreeBag
rpart.plot(fitTreeBag)
str(fitTreeBag)
fancyRpartPlot(fitTreeBag)
fancyRpartPlot(as.rpart(fitTreeBag))
rpart(fitTreeBag)
fitTreeBag
summary(fitTreeBag)
fitTreeBag$varImp
str(fitTreeBag[["varImp"]])
fitTreeBag[["varImp"]]
fitTreeBag[[1]]
fitTreeBag[[2]]
fitTreeBag[[2]][1]
fitTreeBag[[2]]["label"]
fitTreeBag[[2]]["varImp"]
?varImpPlot()
plot(fitNB)
CM
str(CM)
str(CM$byClass)
CM$byClass[,]
CM$byClass[,1:2]
plot(CM$byClass[,1:2])
barplot(CM$byClass[,1:2])
barplot(as.matrix(CM$byClass[,1:2]))
barplot(as.matrix(CM$byClass[,1:2]),beside=T)
barplot(as.matrix(CM$byClass[,1:2]),beside=T,col=colours)
colours <- c("red", "orange", "blue", "yellow", "green")
barplot(as.matrix(CM$byClass[,1:2]),beside=T,col=colours)
library(RColorBrewer)
library(plotly)
py <- plotly()
install.packages("plotly")
library(RColorBrewer)
library(plotly)
py <- plotly()
str(CM$byClass)
names(CM$byClass)
dim(CM$byClass)
names(CM$byClass[,])
data_bar <- list(x = c("Breakfast 1", "Breakfast 2",
"Lunch 1", "Lunch 2",
"Dinner 1", "Dinner 2"),
y = c(7.72, 8.5, 12.22, 14.89, 27.02, 17.23),
type = "bar",
marker = list(color = brewer.pal(6, "Paired"))
)
layout_bar <- list(title = "Price of Meals",
xaxis = list(title = "Meal"),
yaxis = list(title = "Price ($)")
)
library("ggplot2", lib.loc="~/R/win-library/3.1")
ggplot(CM$byClass[,1:2])
ggplot(CM$byClass)
barplot(CM$byClass[,1:2])
barplot(CM$byClass[,1:2],beside=T)
names(CM$byClass)
str(CM$byClass)
CM$byClass[,]
rownames(CM$byClass)
barplot(CM$byClass[,1:2],beside=T,xlab=rownames(CM$byClass))
barplot(CM$byClass[,1:2],beside=T,main="Naive Bayes Sensitivity and Specificity of All Class Predictions",legend=rownames(CM$byClass))
?barplot
barplot(CM$byClass[,1:2],beside=T,main="Naive Bayes Sensitivity and Specificity of All Class Predictions",legend=rownames(CM$byClass),ylim=1)
barplot(CM$byClass[,1:2],beside=T,main="Naive Bayes Sensitivity and Specificity of All Class Predictions",legend=rownames(CM$byClass),ylim=0:1)
barplot(CM$byClass[,1:2],beside=T,main="Naive Bayes Sensitivity and Specificity of All Class Predictions",legend=("topright",legend=rownames(CM$byClass)),ylim=0:1)
barplot(CM$byClass[,1:2],
beside=T,
main="Naive Bayes Sensitivity and Specificity of All Class Predictions",
legend=("topright",legend=rownames(CM$byClass)),
ylim=0:1
)
barplot(CM$byClass[,1:2],
beside=T,
legend=("topright",legend=rownames(CM$byClass)),
main="Naive Bayes Sensitivity and Specificity of All Class Predictions",
ylim=0:1
)
barplot(CM$byClass[,1:2],
beside=T,
legend=("topright"),
main="Naive Bayes Sensitivity and Specificity of All Class Predictions",
ylim=0:1
)
barplot(CM$byClass[,1:2],
beside=T,
legend=("topright",rownames(CM$byClass))),
main="Naive Bayes Sensitivity and Specificity of All Class Predictions",
ylim=0:1
)
barplot(CM$byClass[,1:2],
beside=T,
legend=(rownames(CM$byClass))),
main="Naive Bayes Sensitivity and Specificity of All Class Predictions",
ylim=0:1
)
barplot(CM$byClass[,1:2],
beside=T,
#legend=("topright",rownames(CM$byClass))),
main="Naive Bayes Sensitivity and Specificity of All Class Predictions",
ylim=0:1
)
x=matrix(1:10,2,5)
barplot(x,besid=T, col=c("red","blue"))
legend("topleft", c("left","right"),
fill=c("red","blue"))
x=matrix(1:10,2,5)
barplot(x,besid=T, col=c("red","blue"))
legend("topright", c("left","right"),
fill=c("red","blue"))
barplot(CM$byClass[,1:2],
beside=T,
legend("topright",rownames(CM$byClass))),
main="Naive Bayes Sensitivity and Specificity of All Class Predictions",
ylim=0:1
)
x=matrix(1:10,2,5)
barplot(x,besid=T, col=c("red","blue"))
legend("topright", c("left","right"),
fill=c("red","blue"))
barplot(CM$byClass[,1:2],
beside=T,
main="Naive Bayes Sensitivity and Specificity of All Class Predictions",
ylim=0:1
)
legend("topright",rownames(CM$byClass)))
legend("topright",rownames(CM$byClass))
barplot(CM$byClass[,1:2],
beside=T,
main="Naive Bayes Sensitivity and Specificity of All Class Predictions",
legend("topright",rownames(CM$byClass)),
ylim=0:1
)
barplot(CM$byClass[,1:2],
beside=T,
main="Naive Bayes Sensitivity and Specificity of All Class Predictions",
legend("topright",legend=rownames(CM$byClass)),
ylim=0:1
)
barplot(CM$byClass[,1:2],
beside=T,
main="Naive Bayes Sensitivity and Specificity of All Class Predictions",
legend=rownames(CM$byClass)),
ylim=0:1
)
barplot(CM$byClass[,1:2],
beside=T,
main="Naive Bayes Sensitivity and Specificity of All Class Predictions",
legend=rownames(CM$byClass),
ylim=0:1
)
barplot(CM$byClass[,1:2],
beside=T,
main="Naive Bayes Sensitivity and Specificity of All Class Predictions",
legend(rownames(CM$byClass)),
ylim=0:1
)
?barplot
barplot(CM$byClass[,1:2],
beside=T,
main="Naive Bayes Sensitivity and Specificity of All Class Predictions",
#legend(rownames(CM$byClass)),
ylim=0:1
)
barplot(treeBagCM$byClass[,1:2],
beside=T,
main="Bagged Trees Sensitivity and Specificity of All Class Predictions",
#legend(rownames(CM$byClass)),
ylim=0:1
)
barplot(BoostCM$byClass[,1:2],
beside=T,
main="Boosted trees Sensitivity and Specificity of All Class Predictions",
#legend(rownames(CM$byClass)),
ylim=0:1
)
barplot(neuralNetworkCM$byClass[,1:2],
beside=T,
main="Neural Network Sensitivity and Specificity of All Class Predictions",
#legend(rownames(CM$byClass)),
ylim=0:1
)
CM
treeBagCM
BoostCM
neuralNetworkCM
eval.model
eval.predtrain
eval.predtest<-predict(eval.model,testpredictors)
confTest<-table(Predicted=eval.predtest,Reference=testLabels)
print(confTest[c(2,1),c(2,1)])
confusionMatrix(confTest,positive='TRUE')
library(e1071)
library(caret)
library(NLP)
library(tm)
library(pROC)
library(lattice)
library(ada)
library(rpart)
library(kernlab)
library(ipred)
library(plyr)
library(MASS)
library(pamr)
library(randomForest)
library(ggplot2)
confusionMatrix(confTest,positive='TRUE')
finalCM <- confusionMatrix(confTest,positive='TRUE')
str(finalCM)
str(finalCM$table)
finalCM$table
finalCM$table$Predicted
finalCM$table
colnames(finalCM$table)
input <- finalCM$table
input.matrix <- data.matrix(input)
input.matrix.normalized <- normalize(input.matrix)
?normalize
install.packages("som")
install.packages("som")
input.matrix
input.matrix.normalized <- normalize(input.matrix)
library("som", lib.loc="~/R/win-library/3.1")
input.matrix.normalized <- normalize(input.matrix)
input.matrix.normalized
input.matrix
input.matrix.normalized
colnames(input.matrix.normalized)
rownames(input.matrix.normalized)
confusion <- as.data.frame(as.table(input.matrix.normalized))
plot <- ggplot(confusion)
plot + geom_tile(aes(x=Var1, y=Var2, fill=Freq)) + scale_x_discrete(name="Actual Class") + scale_y_discrete(name="Predicted Class") + scale_fill_gradient(breaks=seq(from=-.5, to=4, by=.2)) + labs(fill="Normalized\nFrequency")
input.matrix.normalized
colnames(input.matrix.normalized) <- rownames(input.matrix.normalized)
input.matrix.normalized
input.matrix.normalized
names(input.matrix.normalized)
str(input.matrix.normalized)
input.matrix
confusion <- as.data.frame(as.table(input.matrix))
plot <- ggplot(confusion)
plot + geom_tile(aes(x=Predicted, y=Reference, fill=Freq)) + scale_x_discrete(name="Actual Class") + scale_y_discrete(name="Predicted Class") + scale_fill_gradient(breaks=seq(from=-.5, to=4, by=.2)) + labs(fill="Normalized\nFrequency")
plot + geom_tile(aes(x=Predicted, y=Reference, fill=value)) + scale_x_discrete(name="Actual Class") + scale_y_discrete(name="Predicted Class") + scale_fill_gradient(breaks=seq(from=-.5, to=4, by=.2)) + labs(fill="Normalized\nFrequency")
plot + geom_tile(aes(x=Predicted, y=Reference)) + scale_x_discrete(name="Actual Class") + scale_y_discrete(name="Predicted Class") + scale_fill_gradient(breaks=seq(from=-.5, to=4, by=.2)) + labs(fill="Normalized\nFrequency")
plot + geom_tile(aes(x=Predicted, y=Reference, fill=Freq)) + scale_x_discrete(name="Actual Class") + scale_y_discrete(name="Predicted Class") + scale_fill_gradient(breaks=seq(from=0, to=500000, by=50)) + labs(fill="Normalized\nFrequency")
plot + geom_tile(aes(x=Predicted, y=Reference, fill=Freq)) + scale_x_discrete(name="Actual Class") + scale_y_discrete(name="Predicted Class") + scale_fill_gradient(breaks=seq(from=0, to=50, by=5)) + labs(fill="Normalized\nFrequency")
input.matrix
plot + geom_tile(aes(x=Predicted, y=Reference, fill=z)) + scale_x_discrete(name="Actual Class") + scale_y_discrete(name="Predicted Class") + scale_fill_gradient(breaks=seq(from=0, to=5, by=1)) + labs(fill="Normalized\nFrequency")
plot + geom_tile(aes(x=Predicted, y=Reference, fill=Freq)) + scale_x_discrete(name="Actual Class") + scale_y_discrete(name="Predicted Class") + labs(fill="Normalized\nFrequency")
plot + geom_tile(aes(x=Predicted, y=Reference, fill=Freq)) + scale_x_discrete(name="Actual Class") + scale_y_discrete(name="Predicted Class") + labs(fill="Frequency")
knnCM$table
input <- knnCM$table
input.matrix <- data.matrix(input)
input.matrix.normalized <- normalize(input.matrix)
colnames(input.matrix.normalized) <- rownames(input.matrix.normalized)
#confusion <- as.data.frame(as.table(input.matrix.normalized))
confusion <- as.data.frame(as.table(input.matrix))
plot <- ggplot(confusion)
plot + geom_tile(aes(x=Predicted, y=Reference, fill=Freq)) + scale_x_discrete(name="Actual Class") + scale_y_discrete(name="Predicted Class") + labs(fill="Frequency")
knnCM$table
input.matrix <- data.matrix(input)
input.matrix.normalized <- normalize(input.matrix)
colnames(input.matrix.normalized) <- rownames(input.matrix.normalized)
#confusion <- as.data.frame(as.table(input.matrix.normalized))
confusion <- as.data.frame(as.table(input.matrix))
plot <- ggplot(confusion)
plot + geom_tile(aes(x=Prediction, y=Reference, fill=Freq)) + scale_x_discrete(name="Actual Class") + scale_y_discrete(name="Predicted Class") + labs(fill="Frequency")
finalCM
barplot(finalCM$byClass[,1:2],
beside=T,
main="Final Model Sensitivity and Specificity of All Class Predictions",
#legend(rownames(CM$byClass)),
ylim=0:1
)
test$hand
test$Hand
table(test$Hand)
plot(table(test$Hand))
barplot(table(test$Hand))
finalCM
knnCM
